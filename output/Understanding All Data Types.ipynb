{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8374e0f1-ff73-4675-92d7-133ab1455c6d",
   "metadata": {},
   "source": [
    "## Data we definitely want to include:\n",
    "### Outdoor polygons: OSM parks and pitches (indoor = No), Parkserve local parks and recreation.\n",
    "* We selected five random cities to get\n",
    "1. Bellingham, Washington — ~93,000\n",
    "2. Sandpoint, Idaho — ~9,000\n",
    "3. Ithaca, New York — ~32,000\n",
    "4. Santa Cruz, California — ~63,000\n",
    "5. Cedar Rapids, Iowa — ~138,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac5b77e-30a9-4aa1-8b20-2919e486be28",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ZipFile\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mosmnx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mox\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shape\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# --- CONFIG ---\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/osmnx/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bearing \u001b[38;5;28;01mas\u001b[39;00m bearing\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert \u001b[38;5;28;01mas\u001b[39;00m convert\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance \u001b[38;5;28;01mas\u001b[39;00m distance\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elevation \u001b[38;5;28;01mas\u001b[39;00m elevation\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m features \u001b[38;5;28;01mas\u001b[39;00m features\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/osmnx/distance.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# scikit-learn is optional dependency for unprojected nearest-neighbor search\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BallTree\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     BallTree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_missing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_indexing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     resample,\n\u001b[1;32m     19\u001b[0m     shuffle,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpx\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/__init__.py:606\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_stats_py.py:49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     52\u001b[0m                                    siegelslopes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/distributions.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_continuous_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_levy_stable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levy_stable\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/stats/_discrete_distns.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entr, logsumexp, betaln, gammaln \u001b[38;5;28;01mas\u001b[39;00m gamln, zeta\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _lazywhere, rng_integers\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floor, ceil, log, exp, sqrt, log1p, expm1, tanh, cosh, sinh\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/interpolate/__init__.py:167\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m========================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInterpolation (:mod:`scipy.interpolate`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m(should not be used in new code).\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fitpack_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# New interface to fitpack library:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_if_needed\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m comb\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fitpack_py\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dfitpack\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_polyint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Interpolator1D\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/interpolate/_fitpack_py.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# These are in the API for fitpack even if not used in fitpack.py itself.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fitpack_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bisplrep, bisplev, dblint  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fitpack_impl \u001b[38;5;28;01mas\u001b[39;00m _impl\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bsplines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BSpline\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scipy/interpolate/_fitpack_impl.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _fitpack\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (atleast_1d, array, ones, zeros, sqrt, ravel, transpose,\n\u001b[1;32m     31\u001b[0m                    empty, iinfo, asarray)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Try to replace _fitpack interface with\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#  f2py-generated version\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1525\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1499\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1631\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:161\u001b[0m, in \u001b[0;36m_path_isfile\u001b[0;34m(path)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:153\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[0;34m(path, mode)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import osmnx as ox\n",
    "from shapely.geometry import shape\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "PLACE_BASE_URL = \"https://www2.census.gov/geo/tiger/TIGER2025/PLACE/tl_2025_{statefp}_place.zip\"\n",
    "CACHE_DIR = \"cache/place\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "STATE_FIPS = {\n",
    "    \"01\": \"Alabama\", \"02\": \"Alaska\", \"04\": \"Arizona\", \"05\": \"Arkansas\",\n",
    "    \"06\": \"California\", \"08\": \"Colorado\", \"09\": \"Connecticut\", \"10\": \"Delaware\",\n",
    "    \"12\": \"Florida\", \"13\": \"Georgia\", \"16\": \"Idaho\", \"17\": \"Illinois\",\n",
    "    \"18\": \"Indiana\", \"19\": \"Iowa\", \"20\": \"Kansas\", \"21\": \"Kentucky\",\n",
    "    \"22\": \"Louisiana\", \"23\": \"Maine\", \"24\": \"Maryland\", \"25\": \"Massachusetts\",\n",
    "    \"26\": \"Michigan\", \"27\": \"Minnesota\", \"28\": \"Mississippi\", \"29\": \"Missouri\",\n",
    "    \"30\": \"Montana\", \"31\": \"Nebraska\", \"32\": \"Nevada\", \"33\": \"New Hampshire\",\n",
    "    \"34\": \"New Jersey\", \"35\": \"New Mexico\", \"36\": \"New York\", \"37\": \"North Carolina\",\n",
    "    \"38\": \"North Dakota\", \"39\": \"Ohio\", \"40\": \"Oklahoma\", \"41\": \"Oregon\",\n",
    "    \"42\": \"Pennsylvania\", \"44\": \"Rhode Island\", \"45\": \"South Carolina\",\n",
    "    \"46\": \"South Dakota\", \"47\": \"Tennessee\", \"48\": \"Texas\", \"49\": \"Utah\",\n",
    "    \"50\": \"Vermont\", \"51\": \"Virginia\", \"53\": \"Washington\", \"54\": \"West Virginia\",\n",
    "    \"55\": \"Wisconsin\", \"56\": \"Wyoming\"\n",
    "}\n",
    "\n",
    "def load_state_places(statefp):\n",
    "    \"\"\"Load (or download + cache) a state's PLACE shapefile as GeoDataFrame.\"\"\"\n",
    "    zip_path = os.path.join(CACHE_DIR, f\"tl_2025_{statefp}_place.zip\")\n",
    "\n",
    "    # Download if not already cached\n",
    "    if not os.path.exists(zip_path):\n",
    "        url = PLACE_BASE_URL.format(statefp=statefp)\n",
    "        print(f\"Downloading {STATE_FIPS[statefp]} PLACE file...\")\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        print(f\"Using cached file for {STATE_FIPS[statefp]}\")\n",
    "\n",
    "    # Load from local zip\n",
    "    return gpd.read_file(f\"zip://{zip_path}\")\n",
    "\n",
    " \n",
    "def get_city_boundary(city_name, state):\n",
    "    \"\"\"\n",
    "    Find and return the boundary GeoDataFrame for a given city within a known U.S. state.\n",
    "    `state` can be the full state name (e.g., \"Colorado\") or FIPS (e.g., \"08\").\n",
    "    \"\"\"\n",
    "    city_name = city_name.lower()\n",
    "\n",
    "    # Resolve statefp from either state name or FIPS code\n",
    "    state_lookup = {v.lower(): k for k, v in STATE_FIPS.items()}\n",
    "    if state.isdigit():\n",
    "        statefp = state.zfill(2)\n",
    "        state_name = STATE_FIPS.get(statefp, f\"FIPS {statefp}\")\n",
    "    else:\n",
    "        statefp = state_lookup.get(state.lower())\n",
    "        if not statefp:\n",
    "            raise ValueError(f\"Unknown state: {state}\")\n",
    "        state_name = state.title()\n",
    "\n",
    "    # Load the state’s PLACE shapefile (cached if available)\n",
    "    gdf = load_state_places(statefp)\n",
    "\n",
    "    # Case-insensitive match for city name\n",
    "    city_rows = gdf[gdf[\"NAME\"].str.lower() == city_name]\n",
    "\n",
    "    if city_rows.empty:\n",
    "        raise ValueError(f\"City '{city_name.title()}' not found in {state_name}.\")\n",
    "\n",
    "    city_rows = city_rows.copy()\n",
    "    city_rows[\"state_name\"] = state_name\n",
    "    return city_rows\n",
    "\n",
    "def load_trails(city, state):\n",
    "    import numpy as np\n",
    "\n",
    "    boundary_gdf = get_city_boundary(city, state)\n",
    "    boundary = boundary_gdf.geometry.iloc[0]\n",
    "\n",
    "    # Step 1 — Broad pull of candidate hiking trails\n",
    "    tags = {\n",
    "        \"highway\": [\"path\"],      # path only, not footway or track\n",
    "    }\n",
    "\n",
    "    trails = ox.features_from_polygon(boundary, tags=tags)\n",
    "    print(\"Raw path features:\", len(trails))\n",
    "\n",
    "    # Clean null geometries\n",
    "    trails = trails[trails.geometry.notnull()].copy()\n",
    "\n",
    "    # Convert polygons → boundary lines\n",
    "    trails[\"geometry\"] = trails[\"geometry\"].apply(\n",
    "        lambda g: g.boundary if g.geom_type in [\"Polygon\", \"MultiPolygon\"] else g\n",
    "    )\n",
    "\n",
    "    # Keep linework\n",
    "    trails = trails[trails.geometry.type.isin([\"LineString\", \"MultiLineString\"])]\n",
    "    print(\"Line geometries:\", len(trails))\n",
    "\n",
    "    # Step 2 — Natural-surface hiking filter\n",
    "    natural_surfaces = {\n",
    "        \"dirt\", \"ground\", \"earth\", \"grass\", \"gravel\", \"fine_gravel\",\n",
    "        \"woodchips\", \"mud\", \"rock\", \"sand\", \"compacted\", \"unpaved\"\n",
    "    }\n",
    "\n",
    "    def is_natural_surface(val):\n",
    "        if isinstance(val, list):\n",
    "            val = val[0]\n",
    "        if val is None:\n",
    "            return False\n",
    "        return str(val).lower() in natural_surfaces\n",
    "\n",
    "    # surfaces allowed OR missing if other hiking cues exist\n",
    "    surface_mask = trails[\"surface\"].apply(is_natural_surface) | trails[\"surface\"].isna()\n",
    "\n",
    "    # Step 3 — Exclude paved / urban paths explicitly\n",
    "    paved_surfaces = {\"asphalt\", \"concrete\", \"paved\", \"cement\", \"paving_stones\"}\n",
    "\n",
    "    def is_paved(val):\n",
    "        if isinstance(val, list):\n",
    "            val = val[0]\n",
    "        if val is None:\n",
    "            return False\n",
    "        return str(val).lower() in paved_surfaces\n",
    "\n",
    "    paved_mask = trails[\"surface\"].apply(is_paved)\n",
    "\n",
    "    # Step 4 — foot allowed OR unspecified  \n",
    "    foot_mask = trails[\"foot\"].isin([\"yes\", \"designated\"]) | trails[\"foot\"].isna()\n",
    "\n",
    "    # Step 5 — bicycle not allowed  \n",
    "    bike_mask = trails[\"bicycle\"].isin([\"no\"]) | trails[\"bicycle\"].isna()\n",
    "\n",
    "    # Step 6 — bonus “hiking-specific” tags\n",
    "    hiking_cues = (trails[\"sac_scale\"].notnull()) | (trails[\"trail_visibility\"].notnull())\n",
    "\n",
    "    # Final mask: must be natural-surface AND foot allowed AND not paved\n",
    "    final_mask = (\n",
    "        surface_mask &\n",
    "        (~paved_mask) &\n",
    "        bike_mask &\n",
    "        foot_mask\n",
    "    ) | hiking_cues\n",
    "\n",
    "    trails = trails[final_mask].copy()\n",
    "    print(\"Filtered hiking trails:\", len(trails))\n",
    "\n",
    "    # Step 7 — Add trail classification\n",
    "    def classify(row):\n",
    "        if pd.notnull(row.get(\"sac_scale\")):\n",
    "            return row[\"sac_scale\"]               # true hiking scale\n",
    "        if is_natural_surface(row.get(\"surface\")):\n",
    "            return \"natural_surface\"\n",
    "        if row.get(\"foot\") in [\"yes\", \"designated\"]:\n",
    "            return \"foot_designated\"\n",
    "        return \"hiking_trail\"\n",
    "\n",
    "    trails[\"trail_type\"] = trails.apply(classify, axis=1)\n",
    "\n",
    "    # Step 8 — Clip to city boundary\n",
    "    if trails.crs != boundary_gdf.crs:\n",
    "        trails = trails.to_crs(boundary_gdf.crs)\n",
    "\n",
    "    trails = gpd.clip(trails, boundary).reset_index(drop=True)\n",
    "\n",
    "    return trails\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0e7c9-df4e-480b-9455-fccd560b166a",
   "metadata": {},
   "source": [
    "### Pull OSM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb88472-dbbd-4705-b2f2-504451180330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OSM data\n",
    "tags = {\n",
    "    \"leisure\": [\"park\", \"pitch\", \"sports_centre\"]\n",
    "}\n",
    "def load_osm(city, state):\n",
    "    city_geom = get_city_boundary(city, state).geometry.iloc[0]\n",
    "    \n",
    "    # Download OSM features that *intersect* the city boundary\n",
    "    gdf_osm = ox.features_from_polygon(city_geom, tags=tags)\n",
    "    \n",
    "    # Ensure correct CRS\n",
    "    gdf_osm = gdf_osm.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    gdf_osm = gdf_osm[gdf_osm.geometry.type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "    # Clip to city boundary (this is the key step!)\n",
    "    gdf_osm_clipped = gdf_osm.clip(city_geom)\n",
    "    #only get polygons\n",
    "    return gdf_osm_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77299b-6dba-4ad5-ba7e-2db50f27a33f",
   "metadata": {},
   "source": [
    "### Pull ParkServe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35981caa-90bb-4d7d-b8a8-b623fdd03085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import json\n",
    "\n",
    "PARKSERVE_URL = (\n",
    "    \"https://server7.tplgis.org/arcgis7/rest/services/\"\n",
    "    \"ParkServe/ParkServe_ProdNew/MapServer/2/query\"\n",
    ")\n",
    "equal_area_crs = \"EPSG:5070\"  # US Albers Equal Area — ideal for area in meters²\n",
    "\n",
    "\n",
    "def get_city_parks_parkserve(city_name, state_name, min_acres=0.1):\n",
    "    \"\"\"\n",
    "    Fetch ParkServe parks that intersect a SINGLE city and return in EPSG:4326.\n",
    "    \n",
    "    Args:\n",
    "        city_name (str): Name of the city (case-insensitive)\n",
    "        state_name (str): Full state name (e.g., \"Washington\") or FIPS code\n",
    "        min_acres (float): Minimum park size in acres to keep\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame (EPSG:4326) ready for Folium plotting\n",
    "    \"\"\"\n",
    "    # --- 1. Get city boundary ---\n",
    "    city_row = get_city_boundary(city_name, state_name).iloc[0]  # Use your existing function\n",
    "    city_geom = city_row.geometry\n",
    "    city_name = city_row[\"NAME\"]\n",
    "\n",
    "    # Build bounding envelope for ArcGIS query\n",
    "    bounds = city_geom.bounds\n",
    "    envelope = {\n",
    "        \"xmin\": bounds[0],\n",
    "        \"ymin\": bounds[1],\n",
    "        \"xmax\": bounds[2],\n",
    "        \"ymax\": bounds[3],\n",
    "        \"spatialReference\": {\"wkid\": 4326}\n",
    "    }\n",
    "\n",
    "    # --- 2. Fetch ParkServe features ---\n",
    "    features = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        params = {\n",
    "            \"where\": \"(park_designation = 'LP' OR park_designation = 'LREC')\",\n",
    "            \"outFields\": \"*\",\n",
    "            \"f\": \"geojson\",\n",
    "            \"geometry\": json.dumps(envelope),\n",
    "            \"geometryType\": \"esriGeometryEnvelope\",\n",
    "            \"spatialRel\": \"esriSpatialRelIntersects\",\n",
    "            \"resultOffset\": offset,\n",
    "            \"resultRecordCount\": 1000,\n",
    "            \"returnGeometry\": \"true\",\n",
    "            \"outSR\": 5070,  # original CRS in meters\n",
    "        }\n",
    "        r = requests.get(PARKSERVE_URL, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        page = r.json().get(\"features\", [])\n",
    "        if not page: break\n",
    "        features.extend(page)\n",
    "        if len(page) < 1000: break\n",
    "        offset += 1000\n",
    "\n",
    "    if not features:\n",
    "        print(f\"No parks found for {city_name}\")\n",
    "        return gpd.GeoDataFrame(columns=[\"park_name\",\"park_designation\",\"geometry\"], crs=\"EPSG:4326\")\n",
    "\n",
    "    # --- 3. Convert features to GeoDataFrame ---\n",
    "    parks = gpd.GeoDataFrame(\n",
    "        [f.get(\"attributes\") or f.get(\"properties\") for f in features if f.get(\"geometry\")],\n",
    "        geometry=[shape(f[\"geometry\"]) for f in features if f.get(\"geometry\")],\n",
    "        crs=\"EPSG:5070\"  # original CRS from ParkServe\n",
    "    )\n",
    "\n",
    "    # Fix invalid geometries\n",
    "    parks[\"geometry\"] = parks.geometry.buffer(0)\n",
    "    parks = parks[~parks.geometry.is_empty]\n",
    "    # --- 4. Clip parks to city boundary ---\n",
    "    parks = parks.to_crs(\"EPSG:4326\")\n",
    "    city_gdf = gpd.GeoDataFrame(geometry=[city_geom.buffer(0)], crs=\"EPSG:4326\")\n",
    "    parks_clipped = gpd.overlay(parks, city_gdf, how=\"intersection\")\n",
    "    \n",
    "    # --- 5. Filter by minimum size ---\n",
    "    parks_clipped = parks_clipped.to_crs(equal_area_crs)\n",
    "    parks_clipped[\"area_acres\"] = parks_clipped.geometry.area / 4046.86\n",
    "    parks_clipped = parks_clipped[parks_clipped[\"area_acres\"] >= min_acres]\n",
    "    parks_clipped = parks_clipped.to_crs(\"EPSG:4326\")  # ready for Folium\n",
    "\n",
    "\n",
    "    # --- 6. Reproject to EPSG:4326 for Folium ---\n",
    "    parks_clipped = parks_clipped.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    print(f\"{city_name}: {len(parks)} parks before clip, {len(parks_clipped)} after clip\")\n",
    "    return parks_clipped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd662c0-6fc7-4735-b41c-bea43ee3d384",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "- To Do: decide wheteher to filter data by minimum size. Some OSM/ParkServe data is very small, and likely not an intential stop. However, it would have a low impact on the final results and we would not have to justify our size cutoff if we left it in.\n",
    "\n",
    "- #### To run, just change the city and state variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9100f-6d7b-4d57-bd03-1c9cd958fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "city = 'Missoula'\n",
    "state = 'Montana'\n",
    "\n",
    "city_geom = get_city_boundary(city,state)\n",
    "center = city_geom.geometry.iloc[0].centroid\n",
    "center = [center.y, center.x]  \n",
    "\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add ParkServe polygons (green)\n",
    "gdf = get_city_parks_parkserve(city, state)\n",
    "folium.GeoJson(\n",
    "    gdf,\n",
    "    name=\"ParkServe\",\n",
    "    style_function=lambda x: {\"color\": \"green\", \"weight\": 1, \"fillOpacity\": 0.1},\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[\"ParkName\"] if \"ParkName\" in gdf.columns else [])\n",
    ").add_to(m)\n",
    "\n",
    "# Add OSM polygons (gray)\n",
    "gdf_osm = load_osm(city, state)\n",
    "folium.GeoJson(\n",
    "    gdf_osm,\n",
    "    name=\"OSM Parks\",\n",
    "    style_function=lambda x: {\"color\": \"gray\", \"weight\": 1, \"fillOpacity\": 0.1},\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[\"osm_category\"] if \"osm_category\" in gdf_osm.columns else [])\n",
    ").add_to(m)\n",
    "\n",
    "# Add OSM trails (orange lines)\n",
    "gdf_trails = load_trails(city, state)\n",
    "\n",
    "folium.GeoJson(\n",
    "    gdf_trails,\n",
    "    name=\"Trails\",\n",
    "    style_function=lambda x: {\"color\": \"orange\", \"weight\": 2, \"opacity\": 0.9},\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"trail_type\"] if \"trail_type\" in gdf_trails.columns else []\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "m.save(f\"{city}_park_overlap.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4d1ab-4d59-45cd-8261-10531809598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb12c88-9f13-4fee-9d02-37c14e33f65f",
   "metadata": {},
   "source": [
    "### Indoor polygons: OSM sports halls, fitness centres. \n",
    "\n",
    "- Average square footage of a fitness studio: https://member.afsfitness.com/content/fitness-studio-fact-sheet\n",
    "- 3,813 ft. --> 61*61 ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89f048-a68d-42a5-9614-2bc5a0ab847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import pandas as pd\n",
    "\n",
    "def load_osm_indoor(city, state):\n",
    "    \"\"\"\n",
    "    Fetch OSM gyms and create ≤61×61 ft indoor zones within building footprints\n",
    "    for point-only gyms, clipped by other business footprints.\n",
    "    \"\"\"\n",
    "    # --- 1. Get city boundary ---\n",
    "    \n",
    "    city_geom = get_city_boundary(city, state)\n",
    "    if city_geom.crs is None or city_geom.crs.to_epsg() != 4326:\n",
    "        city_geom = city_geom.to_crs(\"EPSG:4326\")\n",
    "    city_geom = city_geom.geometry.iloc[0]\n",
    "\n",
    "    # --- 2. Define OSM tags ---\n",
    "    gym_tags = {\"leisure\": [\"fitness_centre\", \"sports_hall\"]}\n",
    "    bldg_tags = {\"building\": True}\n",
    "    business_tags = {\n",
    "        \"amenity\": True, \"shop\": True, \"office\": True,\n",
    "        \"craft\": True, \"tourism\": True, \"leisure\": True\n",
    "    }\n",
    "\n",
    "    # --- 3. Download all OSM features ---\n",
    "    gdf_gym = ox.features_from_polygon(city_geom, tags=gym_tags)\n",
    "    gdf_bldgs = ox.features_from_polygon(city_geom, tags=bldg_tags)\n",
    "    gdf_biz = ox.features_from_polygon(city_geom, tags=business_tags)\n",
    "\n",
    "    # --- 4. Separate gyms ---\n",
    "    gdf_gym_points = gdf_gym[gdf_gym.geometry.geom_type == \"Point\"].copy()\n",
    "    gdf_gym_polys = gdf_gym[gdf_gym.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])].copy()\n",
    "\n",
    "    # --- 5. Project everything once (for meter units) ---\n",
    "    crs_m = \"EPSG:3857\"\n",
    "    gdf_gym_points = gdf_gym_points.to_crs(crs_m)\n",
    "    gdf_gym_polys = gdf_gym_polys.to_crs(crs_m)\n",
    "    gdf_bldgs = gdf_bldgs.to_crs(crs_m)\n",
    "    gdf_biz = gdf_biz.to_crs(crs_m)\n",
    "\n",
    "    gdf_bldgs = gdf_bldgs.reset_index(drop=True)\n",
    "\n",
    "    # --- 6. Spatial joins: assign buildings ---\n",
    "    gyms_in_bldg = gpd.sjoin(\n",
    "    gdf_gym_points,   # <— no column filtering\n",
    "    gdf_bldgs[[\"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "    gyms_in_bldg.rename(columns={\"index_right\": \"building_id\"}, inplace=True)\n",
    "\n",
    "    biz_in_bldg = gpd.sjoin(gdf_biz, gdf_bldgs[[\"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "\n",
    "    # Handle dynamic naming of the building index column\n",
    "    join_col = None\n",
    "    for col in biz_in_bldg.columns:\n",
    "        if col.startswith(\"index_\") or col.endswith(\"_right\"):\n",
    "            join_col = col\n",
    "            break\n",
    "    \n",
    "    if join_col is None:\n",
    "        raise KeyError(\"Could not find building index column in business join result\")\n",
    "    \n",
    "    # Group businesses by building ID\n",
    "    biz_in_bldg.rename(columns={join_col: \"building_id\"}, inplace=True)\n",
    "    biz_grouped = biz_in_bldg.groupby(\"building_id\")\n",
    "\n",
    "    # --- 7. Create indoor zones for point-based gyms ---\n",
    "    zones = []\n",
    "    side_ft = 150\n",
    "    side_m = side_ft * .3048\n",
    "    half = side_m / 2\n",
    "    \n",
    "    for idx, row in gyms_in_bldg.iterrows():\n",
    "        pt = row.geometry\n",
    "        bldg_geom = gdf_bldgs.loc[row[\"building_id\"]].geometry\n",
    "    \n",
    "        # Create 61x61 ft square, clipped to building\n",
    "        x, y = pt.x, pt.y\n",
    "        square = box(x - half, y - half, x + half, y + half)\n",
    "        zone = square.intersection(bldg_geom)\n",
    "    \n",
    "        # Clip by nearby businesses in same building\n",
    "        if row[\"building_id\"] in biz_grouped.groups:\n",
    "            biz_points = biz_grouped.get_group(row[\"building_id\"]).geometry\n",
    "            if not biz_points.empty:\n",
    "                min_dist = biz_points.distance(pt).min()\n",
    "                if pd.notna(min_dist) and min_dist > 0:\n",
    "                    half_dist = min(min_dist / 2.0, half)\n",
    "                    zone = zone.intersection(pt.buffer(half_dist)).intersection(bldg_geom)\n",
    "    \n",
    "        # Preserve attributes from the original point\n",
    "        zones.append({\n",
    "            \"geometry\": zone,\n",
    "            \"gym_name\": row.get(\"name\", None),\n",
    "            \"leisure\": row.get(\"leisure\", None),\n",
    "            \"amenity\": row.get(\"amenity\", None),\n",
    "            \"building_id\": row[\"building_id\"]\n",
    "        })\n",
    "    \n",
    "    gdf_zones = gpd.GeoDataFrame(zones, crs=crs_m)\n",
    "\n",
    "\n",
    "    # --- 8. Combine with polygon gyms (already defined interiors) ---\n",
    "    all_gyms = pd.concat([gdf_zones, gdf_gym_polys], ignore_index=True)\n",
    "    all_gyms = all_gyms.set_crs(crs_m, allow_override=True).to_crs(\"EPSG:4326\")\n",
    "\n",
    "    # --- 9. Clip to city boundary ---\n",
    "    all_gyms = all_gyms.clip(city_geom)\n",
    "\n",
    "    return all_gyms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdb4fc-059c-46a3-ab99-8565637a3b1b",
   "metadata": {},
   "source": [
    "### To Run, just change the city and state variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2630980-ce13-4c72-acf7-caa461547739",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Bellingham'\n",
    "state = 'Washington'\n",
    "\n",
    "city_geom = get_city_boundary(city,state)\n",
    "center = city_geom.geometry.iloc[0].centroid\n",
    "center = [center.y, center.x]  \n",
    "\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add OSM polygons (gray)\n",
    "gdf_osm = load_osm_indoor(city, state)\n",
    "folium.GeoJson(\n",
    "    gdf_osm,\n",
    "    name=\"OSM Parks\",\n",
    "    style_function=lambda x: {\"color\": \"red\", \"weight\": 1, \"fillOpacity\": 0.1},\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[c for c in [\"gym_name\", \"leisure\",\"osm_category\"] if c in gdf_osm.columns])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control (toggle layers)\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m.save(f\"{city}_indoor.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa785b3-6f41-4b70-9edf-a9e54aa99fe6",
   "metadata": {},
   "source": [
    "## Possible Additions: \n",
    "\n",
    "Trailheads and indoor play areas\n",
    "\n",
    "Suggestion: Do not include. There are only 2,000 tagged indoor play areas in the entire world, and over 35,000 places in the US. It is not a large amount of data points to include, especially with the likely small area and focus on children (who don't typically carry phones while playing). Hiking is somewhat more feasible, however, they would need to be grabbed by county instead of city, and they are unevently distributed compared to trails. Moreover, the number of trails in the county depends a lot more on geography compared to city level parks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e4317-7e72-4787-8111-c7fa10f51d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OSM data\n",
    "# --- 2. Define OSM tags ---\n",
    "recreation_tags = {\n",
    "    \"leisure\": [\n",
    "        \"indoor_play\",\n",
    "        \"trailhead\"\n",
    "    ],\n",
    "    \"information\": [\"trailhead\"],\n",
    "    \"highway\": [\"trailhead\"]\n",
    "}\n",
    "\n",
    "def load_osm_uncertain(city, state):\n",
    "    city_geom = get_city_boundary(city, state).geometry.iloc[0]\n",
    "    \n",
    "    # Download OSM features that *intersect* the city boundary\n",
    "    gdf_osm = ox.features_from(city+\",\"+state, tags=recreation_tags)\n",
    "    \n",
    "    # Ensure correct CRS\n",
    "    gdf_osm = gdf_osm.to_crs(\"EPSG:4326\")\n",
    "    #gdf_osm = gdf_osm[gdf_osm.geometry.type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "    # Clip to city boundary (this is the key step!)\n",
    "    #gdf_osm_clipped = gdf_osm.clip(city_geom)\n",
    "    #only get polygons\n",
    "    return gdf_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394d361-a9e1-432b-a5b4-8a0a15d84cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Linn County'\n",
    "state = 'Iowa'\n",
    "\n",
    "city_geom = get_city_boundary('Cedar Rapids', 'Iowa')\n",
    "center = city_geom.geometry.iloc[0].centroid\n",
    "center = [center.y, center.x]  \n",
    "\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add OSM polygons (gray)\n",
    "gdf_osm = load_osm_uncertain(city, state)\n",
    "folium.GeoJson(\n",
    "    gdf_osm,\n",
    "    name=\"OSM Parks\",\n",
    "    style_function=lambda x: {\"color\": \"red\", \"weight\": 1, \"fillOpacity\": 0.1},\n",
    "    tooltip=folium.GeoJsonTooltip(fields=[c for c in [\"name\", \"leisure\", \"landuse\", \"osm_category\", \"highway\", \"information\"] if c in gdf_osm.columns])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control (toggle layers)\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m.save(f\"{city}_indoor.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822a72a-0a82-4946-875c-232ac71eebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to your unified geopackages\n",
    "gpkg_files = glob.glob(\"../unified_geopackages_/state_11_unified.gpkg\")\n",
    "\n",
    "# Target CRS for merging\n",
    "target_crs = \"EPSG:4326\"   # choose EPSG:5070 if you prefer equal-area\n",
    "\n",
    "gdfs = []\n",
    "for gpkg in gpkg_files:\n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg, layer=\"unified_park_area\")\n",
    "\n",
    "        # Skip if no CRS\n",
    "        if gdf.crs is None:\n",
    "            print(f\"⚠️ {gpkg} has no CRS — skipping\")\n",
    "            continue\n",
    "\n",
    "        # Harmonize CRS before concat\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "\n",
    "        gdfs.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {gpkg}: {e}\")\n",
    "\n",
    "# Combine all into one GeoDataFrame\n",
    "usa_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=target_crs)\n",
    "\n",
    "# Optional: filter to lower 48\n",
    "exclude_fips = ['02', '15', '72', '78', '66', '69', '60']\n",
    "if 'state_fips' in usa_gdf.columns:\n",
    "    usa_gdf = usa_gdf[~usa_gdf['state_fips'].isin(exclude_fips)]\n",
    "\n",
    "# Reproject to Web Mercator for basemap\n",
    "usa_gdf = usa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "usa_gdf.plot(ax=ax, color=\"forestgreen\", alpha=1, linewidth=0)\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(\n",
    "    ax,\n",
    "    source=ctx.providers.CartoDB.Positron,\n",
    "    alpha=0.8,\n",
    "    crs=usa_gdf.crs\n",
    ")\n",
    "\n",
    "ax.set_title(\"Unified ParkServe + OSM Park Areas — Lower 48 States\", fontsize=16)\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b83cfaa-6106-4f23-bcfc-50096f372e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_gdf = usa_gdf.to_crs(5070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c65f11-9415-4320-a4e1-b75fc46d2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new area\n",
    "print(usa_gdf[[\"city_name\", \"area_m2\"]].head())\n",
    "print(\"Total area:\", usa_gdf[\"area_m2\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844d95f-191f-4e0f-8434-b42e7b447477",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_gdf[\"area_m2\"] = usa_gdf.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08eb09c-0a35-45a8-8cb9-3391bb262699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usa_gdf[[\"city_name\", \"area_m2\"]].head())\n",
    "print(\"Total area:\", usa_gdf[\"area_m2\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808136f1-43f3-44b5-b4c5-ad686077fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208502cc-8b5d-414b-ad88-3e11974158b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing or invalid geometries\n",
    "usa_gdf = usa_gdf[~usa_gdf.geometry.is_empty & usa_gdf.geometry.notna()].copy()\n",
    "usa_gdf = usa_gdf[usa_gdf.geometry.is_valid].copy()\n",
    "\n",
    "# Optional: ensure polygons only\n",
    "usa_gdf = usa_gdf[usa_gdf.geometry.geom_type.isin([\"Polygon\", \"MultiPolygon\"])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20ea8a-b328-4fb8-9922-5a60f846a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your unified geopackages\n",
    "gpkg_files = glob.glob(\"../unified_geopackages_/state_01_unified.gpkg\")\n",
    "\n",
    "# Load all GeoPackages\n",
    "gdfs = []\n",
    "for gpkg in gpkg_files:\n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg, layer=\"unified_park_area\")\n",
    "        gdfs.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {gpkg}: {e}\")\n",
    "\n",
    "# Combine all into one GeoDataFrame\n",
    "usa_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "\n",
    "# Optional: filter to lower 48 (assuming you have a 'state_fips' or 'state_name' column)\n",
    "# If not, skip this part — it’s only for visualization clarity\n",
    "exclude_fips = ['02', '15', '72', '78', '66', '69', '60']  # AK, HI, PR, VI, GU, MP, AS\n",
    "if 'state_fips' in usa_gdf.columns:\n",
    "    usa_gdf = usa_gdf[~usa_gdf['state_fips'].isin(exclude_fips)]\n",
    "\n",
    "# Reproject to Web Mercator for basemap\n",
    "usa_gdf = usa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "usa_gdf.plot(ax=ax, color=\"forestgreen\", alpha=1, linewidth=0)\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(\n",
    "    ax,\n",
    "    source=ctx.providers.CartoDB.Positron,  # or ctx.providers.Stamen.TerrainBackground\n",
    "    alpha=0.8,\n",
    "    crs=usa_gdf.crs\n",
    ")\n",
    "\n",
    "# Clean up the map\n",
    "ax.set_title(\"Unified ParkServe + OSM Park Areas — Lower 48 States\", fontsize=16)\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a58fca-293e-4ba4-8883-f2714fb4801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your unified geopackages\n",
    "import folium\n",
    "gpkg_files = glob.glob(\"../unified_geopackages_updated/state_*_unified.gpkg\")\n",
    "\n",
    "# Load all GeoPackages\n",
    "gdfs = []\n",
    "for gpkg in gpkg_files:\n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg, layer=\"unified_park_area\")\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", inplace = True,allow_override=True)\n",
    "        gdfs.append(gdf)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {gpkg}: {e}\")\n",
    "\n",
    "# Combine all into one GeoDataFrame\n",
    "\n",
    "usa_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "\n",
    "# Optional: filter to lower 48 (assuming you have a 'state_fips' or 'state_name' column)\n",
    "# If not, skip this part — it’s only for visualization clarity\n",
    "exclude_fips = ['02', '15', '72', '78', '66', '69', '60']  # AK, HI, PR, VI, GU, MP, AS\n",
    "if 'state_fips' in usa_gdf.columns:\n",
    "    usa_gdf = usa_gdf[~usa_gdf['state_fips'].isin(exclude_fips)]\n",
    "group1_fips = {\"30\", \"53\", \"41\", \"38\", \"16\"}  # MT, WA, OR, ND, ID\n",
    "group2_fips = {\"56\", \"49\", \"08\", \"06\", \"32\", \"04\", \"35\"}  # WY, UT, CO, CA, NV, AZ, NM\n",
    "\n",
    "selected_fips = group1_fips | group2_fips\n",
    "\n",
    "usa_gdf_sel = usa_gdf[usa_gdf[\"state_fips\"].astype(str).isin(selected_fips)]\n",
    "print(\"Selected states subset:\", usa_gdf_sel.shape)\n",
    "\n",
    "center = [usa_gdf_sel.geometry.union_all().centroid.y,\n",
    "          usa_gdf_sel.geometry.union_all().centroid.x]\n",
    "# Reproject to Web Mercator for basemap\n",
    "usa_gdf_sel = usa_gdf_sel.to_crs(epsg=3857)\n",
    "\n",
    "m = folium.Map(location=center, zoom_start=6, tiles=\"cartodbpositron\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Add polygons to Folium\n",
    "# -----------------------------\n",
    "folium.GeoJson(\n",
    "    usa_gdf_sel,\n",
    "    name=\"Unified Park Area\",\n",
    "    style_function=lambda x: {\n",
    "        \"color\": \"green\",\n",
    "        \"weight\": 1,\n",
    "        \"fillColor\": \"green\",\n",
    "        \"fillOpacity\": 0.4\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2445e17-c7fa-451c-9479-fb7a459cb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be13aa-6ecb-4257-8141-75d29dd16675",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_gdf.area_m2_parks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047d60d-ecce-4350-baa4-385b41922ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_gdf_sel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f40d3-2b75-4acd-a3f5-4962161b5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "(usa_gdf_sel.area_m2_parks.sum() * 10.78) / 20000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b78c96-6d40-4f85-907a-3cb495aed93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9a7638-6547-48bc-8b06-1b60159908d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming geopackage correctness through a re-run comparison.\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "gpkg_a = \"../unified_geopackages_/state_53_unified.gpkg\"\n",
    "gpkg_b = \"../unified_geopackages_updated/state_53_unified.gpkg\"\n",
    "layer = \"unified_park_area\"\n",
    "\n",
    "cols = [\"city_name\", \"area_m2_parks\", \"geometry\"]  # include geometry\n",
    "gdf_a = gpd.read_file(gpkg_a, layer=layer)[cols]\n",
    "gdf_b = gpd.read_file(gpkg_b, layer=layer)[cols]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b647760b-5669-44f9-9026-a8082ca84602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_by_city(df):\n",
    "    return (\n",
    "        df.groupby(\"city_name\", dropna=False)[\"area_m2_parks\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "area_a = area_by_city(gdf_a).rename(\n",
    "    columns={\"area_m2_parks\": \"area_m2_orig\"}\n",
    ")\n",
    "\n",
    "area_b = area_by_city(gdf_b).rename(\n",
    "    columns={\"area_m2_parks\": \"area_m2_retry\"}\n",
    ")\n",
    "comparison = area_a.merge(\n",
    "    area_b,\n",
    "    on=\"city_name\",\n",
    "    how=\"outer\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "comparison[[\"area_m2_orig\", \"area_m2_retry\"]] = (\n",
    "    comparison[[\"area_m2_orig\", \"area_m2_retry\"]].fillna(0)\n",
    ")\n",
    "\n",
    "comparison[\"diff_m2\"] = (\n",
    "    comparison[\"area_m2_retry\"] - comparison[\"area_m2_orig\"]\n",
    ")\n",
    "\n",
    "comparison[\"diff_pct\"] = (\n",
    "    comparison[\"diff_m2\"] / comparison[\"area_m2_orig\"]\n",
    ").replace([float(\"inf\"), -float(\"inf\")], None) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c3c96b-687c-4604-a8d8-29e7ef02a6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>area_m2_orig</th>\n",
       "      <th>area_m2_retry</th>\n",
       "      <th>_merge</th>\n",
       "      <th>diff_m2</th>\n",
       "      <th>diff_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>West Clarkston-Highland</td>\n",
       "      <td>1.406805e+05</td>\n",
       "      <td>3.257465e+05</td>\n",
       "      <td>both</td>\n",
       "      <td>185066.036233</td>\n",
       "      <td>131.550592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Federal Way</td>\n",
       "      <td>3.297059e+06</td>\n",
       "      <td>3.397507e+06</td>\n",
       "      <td>both</td>\n",
       "      <td>100448.291302</td>\n",
       "      <td>3.046603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Tacoma</td>\n",
       "      <td>8.821719e+06</td>\n",
       "      <td>8.888749e+06</td>\n",
       "      <td>both</td>\n",
       "      <td>67029.352713</td>\n",
       "      <td>0.759822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>2.871840e+07</td>\n",
       "      <td>2.876008e+07</td>\n",
       "      <td>both</td>\n",
       "      <td>41670.891452</td>\n",
       "      <td>0.145102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Pasco</td>\n",
       "      <td>3.582676e+06</td>\n",
       "      <td>3.611681e+06</td>\n",
       "      <td>both</td>\n",
       "      <td>29004.742405</td>\n",
       "      <td>0.809583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Marysville</td>\n",
       "      <td>2.087850e+06</td>\n",
       "      <td>2.084122e+06</td>\n",
       "      <td>both</td>\n",
       "      <td>-3728.044100</td>\n",
       "      <td>-0.178559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Ferndale</td>\n",
       "      <td>4.214217e+05</td>\n",
       "      <td>4.161683e+05</td>\n",
       "      <td>both</td>\n",
       "      <td>-5253.365458</td>\n",
       "      <td>-1.246582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Stevenson</td>\n",
       "      <td>8.191907e+04</td>\n",
       "      <td>7.596418e+04</td>\n",
       "      <td>both</td>\n",
       "      <td>-5954.884898</td>\n",
       "      <td>-7.269229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Picnic Point</td>\n",
       "      <td>8.574816e+05</td>\n",
       "      <td>8.368287e+05</td>\n",
       "      <td>both</td>\n",
       "      <td>-20652.929068</td>\n",
       "      <td>-2.408556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>Vancouver</td>\n",
       "      <td>7.233313e+06</td>\n",
       "      <td>7.212515e+06</td>\n",
       "      <td>both</td>\n",
       "      <td>-20798.039693</td>\n",
       "      <td>-0.287531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city_name  area_m2_orig  area_m2_retry _merge  \\\n",
       "610  West Clarkston-Highland  1.406805e+05   3.257465e+05   both   \n",
       "191              Federal Way  3.297059e+06   3.397507e+06   both   \n",
       "558                   Tacoma  8.821719e+06   8.888749e+06   both   \n",
       "504                  Seattle  2.871840e+07   2.876008e+07   both   \n",
       "436                    Pasco  3.582676e+06   3.611681e+06   both   \n",
       "..                       ...           ...            ...    ...   \n",
       "346               Marysville  2.087850e+06   2.084122e+06   both   \n",
       "194                 Ferndale  4.214217e+05   4.161683e+05   both   \n",
       "543                Stevenson  8.191907e+04   7.596418e+04   both   \n",
       "441             Picnic Point  8.574816e+05   8.368287e+05   both   \n",
       "590                Vancouver  7.233313e+06   7.212515e+06   both   \n",
       "\n",
       "           diff_m2    diff_pct  \n",
       "610  185066.036233  131.550592  \n",
       "191  100448.291302    3.046603  \n",
       "558   67029.352713    0.759822  \n",
       "504   41670.891452    0.145102  \n",
       "436   29004.742405    0.809583  \n",
       "..             ...         ...  \n",
       "346   -3728.044100   -0.178559  \n",
       "194   -5253.365458   -1.246582  \n",
       "543   -5954.884898   -7.269229  \n",
       "441  -20652.929068   -2.408556  \n",
       "590  -20798.039693   -0.287531  \n",
       "\n",
       "[381 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AREA_EPS = 1e-6  # effectively zero\n",
    "\n",
    "diff_cities = comparison[\n",
    "    comparison[\"diff_m2\"].abs() > AREA_EPS\n",
    "].sort_values(\"diff_m2\", ascending=False)\n",
    "\n",
    "diff_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf433b-3429-4c0c-8a66-4a836ec4160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Check CRS\n",
    "print(gdf_a.crs, gdf_b.crs)\n",
    "\n",
    "# Check total area in km²\n",
    "print(gdf_a['geometry'].area.sum() / 1e6)\n",
    "print(gdf_b['geometry'].area.sum() / 1e6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb5349-234d-49d2-ba23-098ccb79b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# Filter for the city\n",
    "city_name = \"Compton\"\n",
    "gdf_a_city = gdf_a[gdf_a['city_name'].str.lower() == city_name.lower()]\n",
    "gdf_b_city = gdf_b[gdf_b['city_name'].str.lower() == city_name.lower()]\n",
    "\n",
    "# Ensure both have geometries\n",
    "if gdf_a_city.empty or gdf_b_city.empty:\n",
    "    raise ValueError(f\"No geometries found for {city_name} in one of the datasets.\")\n",
    "\n",
    "# Convert to WGS84 for Folium\n",
    "gdf_a_city_wgs = gdf_a_city.to_crs(epsg=4326)\n",
    "gdf_b_city_wgs = gdf_b_city.to_crs(epsg=4326)\n",
    "\n",
    "# Compute a difference layer\n",
    "diff_geom = gpd.overlay(gdf_b_city_wgs, gdf_a_city_wgs, how='difference')\n",
    "\n",
    "# Center the map on the city centroid\n",
    "centroid = gdf_a_city_wgs.geometry.unary_union.centroid\n",
    "m = folium.Map(location=[centroid.y, centroid.x], zoom_start=13)\n",
    "\n",
    "# Add original geometry\n",
    "folium.GeoJson(\n",
    "    gdf_a_city_wgs,\n",
    "    name=\"Original Area\",\n",
    "    style_function=lambda x: {\"fillColor\": \"blue\", \"color\": \"blue\", \"fillOpacity\": 0.3, \"weight\":1}\n",
    ").add_to(m)\n",
    "\n",
    "# Add retry geometry\n",
    "folium.GeoJson(\n",
    "    gdf_b_city_wgs,\n",
    "    name=\"Retry Area\",\n",
    "    style_function=lambda x: {\"fillColor\": \"green\", \"color\": \"green\", \"fillOpacity\": 0.3, \"weight\":1}\n",
    ").add_to(m)\n",
    "\n",
    "# Add difference geometry\n",
    "if not diff_geom.empty:\n",
    "    folium.GeoJson(\n",
    "        diff_geom,\n",
    "        name=\"Difference\",\n",
    "        style_function=lambda x: {\"fillColor\": \"red\", \"color\": \"red\", \"fillOpacity\": 0.5, \"weight\":1}\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d4296-339d-47e1-bf8f-90c984252781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
